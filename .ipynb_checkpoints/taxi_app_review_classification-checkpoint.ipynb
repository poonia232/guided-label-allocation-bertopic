{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7f0ceee-6d5c-4aac-8b4e-73f963eaa9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from bertopic import BERTopic\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e909b1e2-dd70-455b-8d21-6ee1cf92105e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9424937-53da-431a-b7b4-bc0fa59b0daa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817f82c5-d704-47b7-809f-dc1a2215b4c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4a7505ea-6e46-4b91-b7f2-954162b4e1cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_name</th>\n",
       "      <th>review_title</th>\n",
       "      <th>review_description</th>\n",
       "      <th>rating</th>\n",
       "      <th>thumbs_up</th>\n",
       "      <th>review_date</th>\n",
       "      <th>developer_response</th>\n",
       "      <th>developer_response_date</th>\n",
       "      <th>appVersion</th>\n",
       "      <th>laguage_code</th>\n",
       "      <th>country_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Google Play</td>\n",
       "      <td>fbc7ffc9-5a89-446e-87fd-d69bf4a7f984</td>\n",
       "      <td>Puipuii Ralte</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The map in Ola is so messed up, i have to pay ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-08-10 16:40:50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.3.2</td>\n",
       "      <td>en</td>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Google Play</td>\n",
       "      <td>5a0051fb-220a-45b2-ba94-a15a2949218f</td>\n",
       "      <td>Deepak Kumar</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Deepak Kumar.... üôèüôèüôèüôèüôè]</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-08-10 16:36:14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Google Play</td>\n",
       "      <td>71ebf933-b734-474d-bb65-a18c90906ed2</td>\n",
       "      <td>Ahamed Azarudeen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Such aa irresponsible app more then I waiting ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-08-10 16:29:31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.3.1</td>\n",
       "      <td>en</td>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Google Play</td>\n",
       "      <td>e1cc0010-60b3-4126-99c2-e8549088566a</td>\n",
       "      <td>Rahil Syed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Worst</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-08-10 15:52:06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0.4</td>\n",
       "      <td>en</td>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Google Play</td>\n",
       "      <td>77cf1be1-b428-4493-ae25-e0f288f79b8f</td>\n",
       "      <td>vin 007</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Too much expensive .. try UBer... They are pro...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-08-10 15:51:10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        source                             review_id         user_name  \\\n",
       "0  Google Play  fbc7ffc9-5a89-446e-87fd-d69bf4a7f984     Puipuii Ralte   \n",
       "1  Google Play  5a0051fb-220a-45b2-ba94-a15a2949218f      Deepak Kumar   \n",
       "2  Google Play  71ebf933-b734-474d-bb65-a18c90906ed2  Ahamed Azarudeen   \n",
       "3  Google Play  e1cc0010-60b3-4126-99c2-e8549088566a        Rahil Syed   \n",
       "4  Google Play  77cf1be1-b428-4493-ae25-e0f288f79b8f           vin 007   \n",
       "\n",
       "   review_title                                 review_description  rating  \\\n",
       "0           NaN  The map in Ola is so messed up, i have to pay ...       1   \n",
       "1           NaN                            Deepak Kumar.... üôèüôèüôèüôèüôè]       5   \n",
       "2           NaN  Such aa irresponsible app more then I waiting ...       1   \n",
       "3           NaN                                              Worst       1   \n",
       "4           NaN  Too much expensive .. try UBer... They are pro...       1   \n",
       "\n",
       "   thumbs_up          review_date developer_response developer_response_date  \\\n",
       "0          0  2023-08-10 16:40:50                NaN                     NaN   \n",
       "1          0  2023-08-10 16:36:14                NaN                     NaN   \n",
       "2          0  2023-08-10 16:29:31                NaN                     NaN   \n",
       "3          0  2023-08-10 15:52:06                NaN                     NaN   \n",
       "4          0  2023-08-10 15:51:10                NaN                     NaN   \n",
       "\n",
       "  appVersion laguage_code country_code  \n",
       "0      6.3.2           en           in  \n",
       "1        NaN           en           in  \n",
       "2      6.3.1           en           in  \n",
       "3      5.0.4           en           in  \n",
       "4        NaN           en           in  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# Read the first 5000 rows from the CSV file\n",
    "# df = pd.read_csv('', nrows=10000)\n",
    "df = pd.read_csv('Ola Customer Reviews.csv', nrows=50000)\n",
    "\n",
    "# Load the dataset\n",
    "# df = pd.read_csv('amazon_uk_shoes_products_dataset_2021_12.csv')\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3eb4f17-28e2-4be8-aed0-3fc92f48848c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# import re\n",
    "# # Load the dataset\n",
    "# # df = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# # Check the data type of the 'review_date' column\n",
    "# print(df['review_date'].dtype)\n",
    "\n",
    "# # Display the first few entries in the 'review_date' column\n",
    "# print(df['review_date'].head())\n",
    "\n",
    "# # Fill NaN values with an empty string to avoid issues during regex extraction\n",
    "# df['review_date'] = df['review_date'].fillna('')\n",
    "\n",
    "# # Convert all values in 'review_date' to strings to ensure consistency\n",
    "# df['review_date'] = df['review_date'].astype(str)\n",
    "\n",
    "# # Extract country using regular expression and create a new column\n",
    "# pattern = r\"Reviewed in ([\\w\\s]+) on\"\n",
    "# df.loc[:, 'country'] = df['review_date'].apply(lambda x: re.search(pattern, x).group(1))\n",
    "\n",
    "# # Count the occurrences of each unique value\n",
    "# counts = df['country'].value_counts()\n",
    "\n",
    "# # Display the counts\n",
    "# print(counts)\n",
    "\n",
    "# # Check for rows with NaN values in 'country'\n",
    "# nan_rows = df[df['country'].isna()]\n",
    "# print(\"Rows with NaN values in 'country':\\n\", nan_rows)\n",
    "\n",
    "# # Display the first few rows to verify\n",
    "# print(df.head())\n",
    "\n",
    "# # Filtering out English only dataset\n",
    "# df = df[df['review_date'].str.contains('United States', na=False)]\n",
    "# print(\"Number of reviews from the United States:\", len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f2bd86ab-ac65-4cd8-a582-98e4a70ce0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the DataFrame after removing NaN values: 38388\n",
      "Number of NaN values in review_text column: 0\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "# Pre-processing 1: Preprocessing function using TensorFlow\n",
    "# \n",
    "# \n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = tf.strings.lower(text)\n",
    "    # Remove HTML tags\n",
    "    text = tf.strings.regex_replace(text, r'<.*?>', '')\n",
    "    # Remove non-alphabet characters\n",
    "    text = tf.strings.regex_replace(text, r'[^a-zA-Z\\s]', '')\n",
    "    # Strip leading and trailing whitespaces\n",
    "    text = tf.strings.strip(text)\n",
    "    return text.numpy().decode('utf-8')\n",
    "\n",
    "# Remove rows with NaN values in the review_text column\n",
    "df = df.dropna(subset=['review_description'])\n",
    "\n",
    "# Select rows where the length of review_description is greater than or equal to 20 characters\n",
    "df = df[df['review_description'].str.len() >= 20]\n",
    "\n",
    "# Check the number of rows in the DataFrame after removing NaN values\n",
    "num_rows = len(df)\n",
    "print(f\"Number of rows in the DataFrame after removing NaN values: {num_rows}\")\n",
    "\n",
    "# Apply preprocessing\n",
    "df['cleaned_review'] = df['review_description'].apply(lambda x: preprocess_text(tf.convert_to_tensor(x)))\n",
    "\n",
    "\n",
    "# Display the first few rows of the dataframe with cleaned reviews\n",
    "df[['review_description', 'cleaned_review']].head()\n",
    "\n",
    "\n",
    "# Check for NaN values in the review_text column\n",
    "nan_count = df['cleaned_review'].isna().sum()\n",
    "print(f\"Number of NaN values in review_text column: {nan_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a78c1ca-6de7-47ee-98b5-9338bd33d174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3483\n",
      "     feature  count\n",
      "0         aa     19\n",
      "1      aadhe      3\n",
      "2        aaj     12\n",
      "3       aane      2\n",
      "4        aap     80\n",
      "...      ...    ...\n",
      "3478   youve      5\n",
      "3479      yr      3\n",
      "3480    zada      4\n",
      "3481    zero     52\n",
      "3482   zyada      4\n",
      "\n",
      "[3483 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "# Pre-processing 2: Train a count vecotrizer to identify stopwords\n",
    "# \n",
    "# \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Vectorize the text using CountVectorizer with n-grams\n",
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "X = vectorizer.fit_transform(df['cleaned_review'])\n",
    "\n",
    "# Display the top n-grams\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(len(feature_names))\n",
    "\n",
    "# Get the feature names (keys)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Sum the counts of each feature across all documents\n",
    "feature_counts = X.toarray().sum(axis=0)\n",
    "\n",
    "# Create a DataFrame to display feature names and their counts\n",
    "feature_df = pd.DataFrame({'feature': feature_names, 'count': feature_counts})\n",
    "print(feature_df)\n",
    "\n",
    "feature_df.to_csv('count_vector.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59ef95d-c5b9-4459-8bf3-8c988d07a0ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "218bdc85-115c-43e0-979d-cc4caf1e740a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rahulpoonia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'over', 'was', 'its', 'thi', 'their', 'are', 'own', 'liye', 'your', 'again', 'kyun', 'then', \"hadn't\", 'mujhe', 'by', 'par', 'hadn', 'where', 'ke', 'rs', 'times', 'tak', 'want', 'did', 'am', 'll', 'than', 'under', 'apne', \"should've\", 'mustn', 'won', 'pathetic', 'aap', 'fir', 'itself', 'up', 'way', 'y', \"you'd\", 'above', 'had', 'best', 'like', 'through', 'do', 've', 'aapne', 'he', 'so', 'you', 'himself', 'from', 'having', \"haven't\", 'too', 'yourself', 'kya', 'pr', 'diya', 'rahe', 'some', 'wrost', 'aren', 'kuch', 'nor', 'to', 'or', 'hasn', 'hona', 'use', \"you've\", 'phir', 'for', 'yourselves', 'nahi', 'they', \"she's\", \"aren't\", 'hi', 'between', 'whom', 'ko', 'karne', 'why', 'once', 'will', 'ni', 'few', 'pe', 'hota', 'before', 'koi', 'into', \"wasn't\", 'bad', 'didnt', 'dete', 'the', \"hasn't\", 'after', 'being', 'ye', 'ourselves', 'de', 'karta', 'any', 'my', 'has', 'o', 'more', \"didn't\", 'when', 'his', 'hu', 'tabhi', 'same', 'and', 'our', 'on', 'does', 'very', 'until', 'should', 'against', 'in', 'worst', 'shouldn', 'there', 'herself', \"needn't\", 'kabhi', 'an', \"doesn't\", 'further', 'im', 'at', 'couldn', 'hers', 'wale', 'kiya', 'what', 'can', 'm', 'we', 'each', 'mere', 'as', 'better', \"won't\", 'ki', 'most', 'bakwas', 'no', 'myself', 'kar', 'of', 'if', 'nice', 'na', 'don', 'mightn', 'while', 'wont', 'been', 'raha', 'ola', 'hain', \"weren't\", \"couldn't\", \"shouldn't\", 'ours', 'karna', 'here', 'out', 'not', 'be', 'needn', 't', 'bekar', 'mein', 'baad', 'got', 'weren', 'themselves', \"mightn't\", 'se', 'a', 'about', \"wouldn't\", 'them', 'those', 'main', 'this', 'd', 'because', \"mustn't\", 'ek', 'didn', 'bahut', 'hai', 'ha', 'using', 'jab', 'ho', 'rahi', 'both', 'him', 'shan', 'other', 'wouldn', 'only', 'bhi', 'wasn', 'haven', 'with', 'gaya', 'off', 'mera', 'is', 'nhi', \"you're\", \"don't\", 'how', 'karte', 'doesn', 'theirs', 'these', 'during', 'it', 'kam', 'good', 'she', 'ne', 'sabse', 'me', 'i', 'isn', 'below', 'which', 'mene', \"isn't\", 'yours', \"it's\", 're', 'were', 'doing', 'all', 'ka', 'tha', 'that', 'tab', 'her', 'such', 'now', 'ghatiya', 'bar', \"shan't\", \"you'll\", 'ain', 'but', 'aur', 'kr', 'ma', 'karo', 'have', 'tum', 'hun', 'le', 'unki', 'down', 'jana', 'just', 's', 'jo', 'dont', 'who', \"that'll\"}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Initialize the WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Download stop words\n",
    "nltk.download('stopwords')\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Sample Hinglish stopwords list (you can expand this list)\n",
    "hinglish_stopwords = {\n",
    "    'hai', 'ho', 'main', 'mein', 'tum', 'kya', 'kyun', 'ke', 'ko', 'se', 'ka', 'ki', 'par', \n",
    "    'ek', 'aur', 'aur', 'hi', 'kar', 'raha', 'rahe', 'rahi', 'tha', 'thi', 'the', 'hai', \n",
    "    'hun', 'hona', 'diya', 'gaya', 'ke', 'ko', 'ne', 'se', 'me', 'ka', 'ki', 'ho', 'kyun',\n",
    "    'nahi', 'nhi', 'bhi', 'koi', 'kiya', 'ye', 'aap', 'na', 'hota', 'jab', 'tab', 'bahut',\n",
    "    'dete', 'bar', 'karte', 'kabhi', 'tabhi', 'karo', 'kam', 'kr', 'pe', 'phir', 'sabse',\n",
    "    'best', 'good', 'worst', 'ola', 'nice', 'wrost', 'ghatiya', 'bekar', 'bakwas', 'fir', 'baad',\n",
    "    'hain', 'mujhe', 'le', 'jana', 'liye', 'hu', 'de', 'karta', 'karne', 'wale', 'karna', 'ni', 'mera',\n",
    "    'kuch', 'ha', 'tak', 'aapne', 'main', 'apne', 'jo', 'mene', 'mein', 'pr', 'mere',\n",
    "    \"ola\", \"worst\", \"dont\", \"bad\", \"use\", \"rs\", \"just\", \"times\", \"pathetic\",\n",
    "    \"hai\", \"good\", \"way\", \"didnt\", \"better\", \"want\", \"using\", \"got\", \"like\",\n",
    "    \"im\", \"se\", \"wont\", \"ke\", \"nhi\", \"aap\", \"kar\", \"does\", \"bhi\", \"nahi\", \"ka\",\n",
    "    \"hi\", \"ki\", \"unki\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Get English stopwords from NLTK\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# Combine Hinglish and English stopwords\n",
    "combined_stopwords = hinglish_stopwords.union(english_stopwords)\n",
    "\n",
    "print(combined_stopwords)\n",
    "\n",
    "\n",
    "\n",
    "# Function to remove stop words and lemmatize documents\n",
    "def remove_stopwords(doc):\n",
    "    tokens = doc.split()\n",
    "    tokens = [word for word in tokens if word.lower() not in combined_stopwords]\n",
    "    tokens = [lemmatizer.lemmatize(word, pos=wordnet.VERB) for word in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Remove stop words from the reviews\n",
    "df['cleaned_review'] = df['cleaned_review'].apply(remove_stopwords)\n",
    "# Select rows where the length of review_description is greater than or equal to 100 characters\n",
    "df = df[df['cleaned_review'].str.len() >= 20]\n",
    "# # Add a new column 'review_length' that contains the length of each review\n",
    "# # df['review_length'] = df['cleaned_review'].str.len()\n",
    "# print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "25b4217c-6d19-4f83-bf12-8231c3c63411",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "# from bertopic import BERTopic\n",
    "import hdbscan\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Create a HDBSCAN model with adjusted parameters\n",
    "# hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=10, min_samples=2, metric='euclidean')\n",
    "hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=20, \n",
    "                                min_samples=5, metric='euclidean', \n",
    "                                cluster_selection_method='eom', prediction_data=True)\n",
    "\n",
    "\n",
    "# Customize UMAP parameters\n",
    "umap_model = umap.UMAP(n_neighbors=5, n_components=5,\n",
    "                       min_dist=0.0, metric='cosine', low_memory=False)\n",
    "\n",
    "# Initialize KMeans with a predefined number of clusters\n",
    "num_clusters = 32\n",
    "kmeans_model = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6326f7f0-4799-412a-8ec6-cf512ef38058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Anchors designed to nudge the model towards measuring specific genres\n",
    "# # anchors = [\n",
    "# #     [\"comfortable\", \"fit\"],\n",
    "# #     [\"quite\", \"sale\"]\n",
    "# # ]\n",
    "\n",
    "# # seed_topics = {\n",
    "# #     0: [\"great\", \"love\", \"recommend\", \"excellent\"],\n",
    "# #     1: [\"interface\", \"user-friendly\", \"slow\", \"improvement\"],\n",
    "# #     2: [\"worst\", \"crashes\", \"bugs\", \"issues\"]\n",
    "# # }\n",
    "\n",
    "# import numpy as np\n",
    "# print(np.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66819ca5-f178-4fbd-9f57-4eb3fc9f5091",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84506fe6-6a5a-4ca1-a54a-ac8f5698460d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca714fb-caee-440f-b791-638c933037cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a17262e9-5389-4fd3-8ce2-604f93ccc254",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# from bertopic import BERTopic\n",
    "# import numpy as np\n",
    "\n",
    "# # Example DataFrame\n",
    "# df = pd.DataFrame({\n",
    "#     'cleaned_review': [\n",
    "#         \"This is a great app. I love using it every day.\",\n",
    "#         \"The interface is not user-friendly and it's slow.\",\n",
    "#         \"Excellent features and performance. Highly recommended!\",\n",
    "#         \"Needs improvement in terms of speed and usability.\",\n",
    "#         \"Worst app ever. Crashes frequently and too many bugs.\"\n",
    "#     ]\n",
    "# })\n",
    "\n",
    "# # Extract the cleaned reviews as a list\n",
    "# documents = df['cleaned_review'].tolist()\n",
    "\n",
    "# # Define seed topics as a list of lists\n",
    "# seed_topics = [\n",
    "#     [\"great\", \"love\", \"recommend\", \"excellent\"],  # Topic 0\n",
    "#     [\"interface\", \"user-friendly\", \"slow\", \"improvement\"],  # Topic 1\n",
    "#     #[\"worst\", \"crashes\", \"bugs\", \"issues\"]  # Topic 2\n",
    "# ]\n",
    "\n",
    "# # Load a pre-trained sentence embedding model\n",
    "# embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# # Compute embeddings for all documents\n",
    "# embeddings = embedding_model.encode(documents, show_progress_bar=True)\n",
    "\n",
    "# # Verify the shape of the document embeddings\n",
    "# print(f\"Document Embeddings Shape: {embeddings.shape}\")\n",
    "\n",
    "# # Compute embeddings for the seed topics\n",
    "# seed_topic_embeddings = [embedding_model.encode(\" \".join(seed_topic)) for seed_topic in seed_topics]\n",
    "\n",
    "# # Convert list of seed topic embeddings to a numpy array and verify shape\n",
    "# seed_topic_embeddings = np.array(seed_topic_embeddings)\n",
    "# print(f\"Seed Topic Embeddings Shape: {seed_topic_embeddings.shape}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d4df88d-e1ef-4a1c-82d4-da1b6b3d1c38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Initialize BERTopic with the custom embedding model and seed topics\n",
    "# topic_model = BERTopic(embedding_model=embedding_model, seed_topic_list=seed_topics)\n",
    "# # topic_model = BERTopic(seed_topic_list=seed_topic_embeddings)\n",
    "\n",
    "# # Fit the model to your documents using precomputed embeddings\n",
    "# topics, probabilities = topic_model.fit_transform(documents = df['cleaned_review'])\n",
    "\n",
    "# # Display the most frequent topics\n",
    "# print(topic_model.get_topic_freq())\n",
    "\n",
    "# # Get the words and their c-TF-IDF scores for a specific topic\n",
    "# topic_number = 0\n",
    "# print(topic_model.get_topic(topic_number))\n",
    "\n",
    "# # Visualize the topics\n",
    "# topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f336e466-aa85-433d-9ab1-b6e5c4316b2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "     Topic  Count                                     Name  \\\n",
      "0       -1  14592                -1_ride_money_book_driver   \n",
      "1        0   1434                    0_paise_krte_krne_mai   \n",
      "2        1    603                1_postpaid_post_dues_bill   \n",
      "3        2    553  2_rude_behaviour_unprofessional_drivers   \n",
      "4        3    327             3_pending_still_payment_cash   \n",
      "..     ...    ...                                      ...   \n",
      "313    312     20         312_threaten_spam_whatsapp_legal   \n",
      "314    313     20    313_chat_article_resolevd_repeatative   \n",
      "315    314     20             314_km_frad_lalbaug_lessthan   \n",
      "316    315     20                315_upi_id_validate_paytm   \n",
      "317    316     20     316_rapido_uber_administration_touch   \n",
      "\n",
      "                                        Representation  \\\n",
      "0    [ride, money, book, driver, drivers, pay, ask,...   \n",
      "1    [paise, krte, krne, mai, jyada, sab, toh, rha,...   \n",
      "2    [postpaid, post, dues, bill, pay, deactivate, ...   \n",
      "3    [rude, behaviour, unprofessional, drivers, beh...   \n",
      "4    [pending, still, payment, cash, pay, already, ...   \n",
      "..                                                 ...   \n",
      "313  [threaten, spam, whatsapp, legal, call, tortur...   \n",
      "314  [chat, article, resolevd, repeatative, message...   \n",
      "315  [km, frad, lalbaug, lessthan, olain, parel, bu...   \n",
      "316  [upi, id, validate, paytm, verification, visib...   \n",
      "317  [rapido, uber, administration, touch, shame, s...   \n",
      "\n",
      "                                   Representative_Docs  \n",
      "0    [service please take charge extra money driver...  \n",
      "1    [ride paise extend deta aapko paise dikhakr ri...  \n",
      "2    [unable pay postpaid bill, happen money postpa...  \n",
      "3    [customer support drivers rude, experience rud...  \n",
      "4    [already pay auto driver still show pending am...  \n",
      "..                                                 ...  \n",
      "313  [constantly harass sms call make online paymen...  \n",
      "314  [support option chat executive service, custom...  \n",
      "315  [show km reach destination show, app km fare s...  \n",
      "316  [add upi id atleast money deduct yet cannot ge...  \n",
      "317  [also become incompetent greedy drivers take s...  \n",
      "\n",
      "[318 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "## Fit BERTopic model\n",
    "\n",
    "\n",
    "# topic_model = BERTopic(calculate_probabilities=True)\n",
    "# topic_model = BERTopic(calculate_probabilities=True, min_topic_size = 5)\n",
    "# topic_model = BERTopic(calculate_probabilities=True, language = \"multilingual\")\n",
    "# topic_model = BERTopic(calculate_probabilities=True, umap_model=umap_model)\n",
    "# topic_model = BERTopic(seed_topic_list = seed_topics)\n",
    "topic_model = BERTopic(calculate_probabilities=True, umap_model=umap_model, hdbscan_model = hdbscan_model)\n",
    "# topic_model = BERTopic(nr_topics=20)\n",
    "# topic_model = BERTopic(min_topic_size = 30)\n",
    "\n",
    "# topic_model = BERTopic(min_topic_size=4, n_gram_range=(1, 4), calculate_probabilities=True, similarity_threshold=0.0)\n",
    "# topics, probs = topic_model.fit_transform(df['cleaned_review'], embeddings=document_embeddings)\n",
    "topics, probs = topic_model.fit_transform(df['cleaned_review'])\n",
    "\n",
    "# print(len(topics))\n",
    "print(isinstance(topics, str))\n",
    "\n",
    "# print(topic_model.topics_)\n",
    "\n",
    "# Add topics to the original DataFrame\n",
    "# df['topic'] = pd.DataFrame(topic_model.topics_, columns=['Topic'])\n",
    "\n",
    "df['topic'] = pd.DataFrame(topic_model.topics_, columns=['Topic'])\n",
    "\n",
    "# print(df['topic'])\n",
    "\n",
    "# Get topic info\n",
    "topic_info = topic_model.get_topic_info()\n",
    "print(topic_info)\n",
    "\n",
    "# df['topic'].head\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Extract the topics and their representative words\n",
    "topics_list = []\n",
    "for topic in topic_info['Topic']:\n",
    "    if topic != -1:  # Ignore outliers\n",
    "        topic_words = topic_model.get_topic(topic)\n",
    "        topic_words_str = \", \".join([word[0] for word in topic_words])\n",
    "        topics_list.append({\"Topic\": topic, \"Words\": topic_words_str})\n",
    "\n",
    "# Convert to DataFrame\n",
    "topics_df = pd.DataFrame(topics_list)\n",
    "\n",
    "# Save to CSV\n",
    "topics_df.to_csv(\"ola_topics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2b9d08bc-811a-4694-87de-901f96c76c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Topic  Count                                               Name  \\\n",
      "0       -1  14592                            -1_ride_driver_app_book   \n",
      "1        0  12461                           0_ride_cancel_app_driver   \n",
      "2        1   1434                           1_paise_driver_book_krte   \n",
      "3        2    603                2_postpaid_pay_money_money postpaid   \n",
      "4        3    294               3_charge_twice_charge twice_customer   \n",
      "..     ...    ...                                                ...   \n",
      "101    100     20           100_km_show_show km_km reach destination   \n",
      "102    101     20                   101_safe_women_safety_safe women   \n",
      "103    102     20  102_technical_glitch_show technical_technical ...   \n",
      "104    103     20                103_chat_support_option_chat option   \n",
      "105    104     20                        104_auto_book auto_book_get   \n",
      "\n",
      "                                        Representation  \\\n",
      "0    [ride, driver, app, book, drivers, cancel, ser...   \n",
      "1    [ride, cancel, app, driver, drivers, book, cha...   \n",
      "2    [paise, driver, book, krte, krne, log, mai, ap...   \n",
      "3    [postpaid, pay, money, money postpaid, bill, p...   \n",
      "4    [charge, twice, charge twice, customer, servic...   \n",
      "..                                                 ...   \n",
      "101  [km, show, show km, km reach destination, km r...   \n",
      "102  [safe, women, safety, safe women, girls, women...   \n",
      "103  [technical, glitch, show technical, technical ...   \n",
      "104  [chat, support, option, chat option, chat supp...   \n",
      "105  [auto, book auto, book, get, get auto, wait, m...   \n",
      "\n",
      "                                   Representative_Docs  \n",
      "0    [experience every time try book ride driver ca...  \n",
      "1    [customer service horrible select upi payment ...  \n",
      "2    [bhut bhut jada ganda es wjhe bhut pareshani h...  \n",
      "3    [scam app pay cash driver also get money postp...  \n",
      "4    [double charge charge twice refund money fraud...  \n",
      "..                                                 ...  \n",
      "101  [experience today today travel km app show km ...  \n",
      "102  [service safe service, really safe women since...  \n",
      "103  [book show technical glitch error, start book ...  \n",
      "104  [support option chat executive service, custom...  \n",
      "105  [past week auto get book even wait minutes get...  \n",
      "\n",
      "[106 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# reduce topics for it to make some sense, the original number of topics are simply too high\n",
    "topic_model.reduce_topics(df['cleaned_review'], nr_topics='auto')\n",
    "\n",
    "\n",
    "# Update topic representation by increasing n-gram range and removing english stopwords\n",
    "topic_model.update_topics(df['cleaned_review'], topic_model.topics_, n_gram_range=(1, 3))\n",
    "\n",
    "# Get topic info\n",
    "topic_info = topic_model.get_topic_info()\n",
    "print(topic_info)\n",
    "\n",
    "\n",
    "# Extract the topics and their representative words\n",
    "topics_list = []\n",
    "for topic in topic_info['Topic']:\n",
    "    if topic != -1:  # Ignore outliers\n",
    "        topic_words = topic_model.get_topic(topic)\n",
    "        topic_words_str = \", \".join([word[0] for word in topic_words])\n",
    "        topics_list.append({\"Topic\": topic, \"Words\": topic_words_str})\n",
    "\n",
    "\n",
    "# put the words in a column next to topic\n",
    "topics_list = []\n",
    "for topic in topic_model.topics_:\n",
    "    if topic != -1:  # Ignore outliers\n",
    "        topic_words = topic_model.get_topic(topic)\n",
    "        topic_words_str = \", \".join([word[0] for word in topic_words])\n",
    "        topics_list.append({\"Topic\": topic, \"Words\": topic_words_str})\n",
    "\n",
    "    elif topic == -1:\n",
    "        topic_words_str = \"Un-clustered\"\n",
    "        topics_list.append({\"Topic\": topic, \"Words\": topic_words_str})\n",
    "\n",
    "\n",
    "\n",
    "# Create the DataFrame from the list of dictionaries\n",
    "df_topics = pd.DataFrame(topics_list)\n",
    "\n",
    "\n",
    "df.to_csv('ola_df.csv',index = False)\n",
    "df_topics.to_csv('ola_bertopics_words.csv',index = False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d84e7ee-32fb-4119-a210-dacaf72c1b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # \n",
    "# # Pre-processing 3: Use the list of topics identified in the above steps to find a list of anchor words to get topics like you want\n",
    "# # \n",
    "# # \n",
    "\n",
    "# seed_topic_list = [[\"drug\", \"cancer\", \"drugs\", \"doctor\"],\n",
    "#                    [\"windows\", \"drive\", \"dos\", \"file\"],\n",
    "#                    [\"space\", \"launch\", \"orbit\", \"lunar\"]]\n",
    "\n",
    "\n",
    "# topic_model = BERTopic(seed_topic_list=seed_topic_list)\n",
    "# topics, probs = topic_model.fit_transform(df['cleaned_review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "639b01c3-0c4f-4799-b068-c95e6ab880e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34875, 15)\n",
      "(34875, 2)\n",
      "(34875, 17)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(df_topics.shape)\n",
    "\n",
    "# Reset the index for both DataFrames\n",
    "df_reset = df.reset_index(drop=True)\n",
    "df_topics_reset = df_topics.reset_index(drop=True)\n",
    "\n",
    "# Concatenate the DataFrames along the columns (axis=1)\n",
    "df_combined = pd.concat([df_reset, df_topics_reset], axis=1)\n",
    "\n",
    "# Select the desired columns, for example, 'column1', 'column2', and 'column3'\n",
    "selected_columns = df_combined[['review_description', 'rating', 'Words']]\n",
    "\n",
    "# Save the selected columns to a CSV file\n",
    "selected_columns.to_csv('selected_columns.csv', index=False)\n",
    "\n",
    "# Verify the shape of the resulting DataFrame\n",
    "print(df_combined.shape)  # Should print (34875, 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f9cb3b-d394-4189-99f4-f626c445414a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "62de0ef9-c423-4b48-b13c-b57309e09e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: ['ride', 'driver', 'app', 'drivers', 'cancel', 'cab', 'able', 'destination', 'book', 'just']\n",
      "Topic 1: ['driver', 'ola', 'post', 'given', 'paid', 'payment', 'ride', 'worst', 'bad', 'app']\n",
      "Topic 2: ['application', 'app', 'ola', 'worst', 'scam', 'customer', 'nice', 'showing', 'care', 'extra']\n",
      "Topic 3: ['uber', 'ola', 'better', 'app', 'far', 'driver', 'expensive', 'use', 'km', 'dont']\n",
      "Topic 4: ['price', 'ride', 'booking', 'ola', 'fare', 'destination', 'shows', 'time', 'worst', 'app']\n",
      "Topic 5: ['point', 'company', 'drivers', 'app', 'fraud', 'ola', 'doesnt', 'worst', 'pickup', 'pick']\n",
      "Topic 6: ['ola', 'booked', 'cab', 'driver', 'cabs', 'charged', 'auto', 'ride', 'complaint', 'didnt']\n",
      "Topic 7: ['good', 'ola', 'app', 'login', 'drivers', 'experience', 'service', 'dont', 'services', 'just']\n",
      "Topic 8: ['route', 'driver', 'double', 'charge', 'fare', 'app', 'ola', 'charges', 'extra', 'ride']\n",
      "Topic 9: ['app', 'account', 'blocked', 'reason', 'support', 'customer', 'location', 'high', 'booking', 'dont']\n",
      "Topic 10: ['rs', 'ride', 'ola', 'app', 'booked', 'pay', 'fare', 'book', 'showing', 'charge']\n",
      "Topic 11: ['ride', 'cancel', 'drivers', 'driver', 'time', 'accept', 'waiting', 'ask', 'rides', 'accepting']\n",
      "Topic 12: ['ola', 'payment', 'paid', 'cash', 'money', 'driver', 'pay', 'customer', 'app', 'postpaid']\n",
      "Topic 13: ['customer', 'support', 'service', 'care', 'ola', 'worst', 'contact', 'number', 'pathetic', 'drivers']\n",
      "Topic 14: ['ride', 'driver', 'charged', 'cancellation', 'fare', 'fee', 'cancelled', 'app', 'support', 'charge']\n",
      "Topic 15: ['bad', 'ola', 'experience', 'drivers', 'driver', 'service', 'auto', 'money', 'dont', 'extra']\n",
      "Topic 16: ['ola', 'service', 'customer', 'app', 'dont', 'use', 'time', 'care', 'company', 'worst']\n",
      "Topic 17: ['app', 'use', 'ola', 'worst', 'dont', 'driver', 'experience', 'uber', 'issue', 'just']\n",
      "Topic 18: ['ola', 'rides', 'app', 'drivers', 'day', 'booking', 'months', 'pay', 'bed', 'small']\n",
      "Topic 19: ['location', 'drop', 'ola', 'driver', 'ghatiya', 'drivers', 'service', 'worst', 'best', 'app']\n",
      "Topic 20: ['app', 'time', 'waste', 'worst', 'ola', 'working', 'dont', 'wrost', 'properly', 'driver']\n",
      "Topic 21: ['money', 'extra', 'ola', 'drivers', 'asking', 'ask', 'bike', 'charges', 'time', 'driver']\n",
      "Topic 22: ['hai', 'se', 'ke', 'nhi', 'kar', 'bhi', 'nahi', 'ka', 'ki', 'hi']\n",
      "Topic 23: ['worst', 'app', 'service', 'drivers', 'ola', 'time', 'experience', 'driver', 'cab', 'booking']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Apply LDA to extract aspects\n",
    "n_topics = 24  # Adjust the number of topics as needed\n",
    "lda_model = LatentDirichletAllocation(n_components=n_topics, random_state=0)\n",
    "lda_matrix = lda_model.fit_transform(X)\n",
    "\n",
    "# Display the top n-grams for each topic\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    topics = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topics[topic_idx] = [feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "    return topics\n",
    "\n",
    "no_top_words = 10\n",
    "topics = display_topics(lda_model, vectorizer.get_feature_names_out(), no_top_words)\n",
    "for topic, words in topics.items():\n",
    "    print(f\"Topic {topic}: {words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41301a1c-2260-4ea2-8299-0579a5d962bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
